{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa29a44e",
   "metadata": {},
   "source": [
    "### English to Bahasa Translation using SeaLion-v3-9b-IT instruction tuned LLM based on Gemma2\n",
    "### This is run in Runpod/remote Jupyter environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4953a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers safetensors sentencepiece huggingface-hub accelerate bitsandbytes tqdm openai backoff retrying protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/aisingapore/Gemma-SEA-LION-v3-9B-IT LLM_MODELS/Gemma-SEA-LION-v3-9B-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f65d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL_MODEL_PATH = aisingapore/Gemma-SEA-LION-v3-9B-IT\n",
      "LLM_MODEL = aisingapore/Gemma-SEA-LION-v3-9B-IT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "######### Also useful to reduce thread contention:\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "snapshot_path = \"/workspace/LLM_MODELS/Gemma-SEA-LION-v3-9B-IT\"\n",
    "snapshot_path = \"aisingapore/Gemma-SEA-LION-v3-9B-IT\"\n",
    "\n",
    "os.environ[\"LOCAL_MODEL_PATH\"] = snapshot_path\n",
    "os.environ[\"LLM_MODEL\"] = snapshot_path\n",
    "\n",
    "######## enable 4-bit for quants (and bitsandbytes is set up)\n",
    "os.environ[\"LLM_LOAD_IN_4BIT\"] = \"1\"  # or \"0\" to disable quantization\n",
    "print(\"LOCAL_MODEL_PATH =\", os.environ[\"LOCAL_MODEL_PATH\"])\n",
    "print(\"LLM_MODEL =\", os.environ[\"LLM_MODEL\"])\n",
    "\n",
    "### If kernel doesnt recognize\n",
    "LLM_MODEL=snapshot_path\n",
    "LOCAL_MODEL_PATH=snapshot_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b28d55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_WORKER_MAX_TIME = 300\n",
      "MAX_NEW_TOKENS = 700\n",
      "BATCH_NUM = 1\n"
     ]
    }
   ],
   "source": [
    "# Commandline args universal\n",
    "# MAX_NEW_TOKENS is purely for text generation count limit while max_position_embeddings is for context_length based on LLM config.json. !!! input_length + MAX_NEW_TOKENS shopuld be < context_length, otherwise LLM breaks. Llama 3 only has 8k context length/max_posiiton_embedding. SEALIONv3-LLama3-8B-IT uses ROPE, max_position_embeddings follows ROPE limit 131k, Qwen2.5-7B-IT has 32k context length, SahabatAIv1-LLama3-8B-IT has 8k context length.\n",
    "# Counted the response for each steps in notebook output cell with tokens counter online, translations ~400 tokens, ~decomposition ~500 tokens, search_resolve ~700 tokens\n",
    "# Change this every process (translate, decompose, search_resolve), different value is needed. Time is in seconds.\n",
    "os.environ[\"LLM_WORKER_MAX_TIME\"] = \"300\"\n",
    "LLM_WORKER_MAX_TIME=300\n",
    "os.environ[\"MAX_NEW_TOKENS\"] = \"700\"\n",
    "MAX_NEW_TOKENS=700\n",
    "os.environ[\"BATCH_NUM\"] = \"1\"\n",
    "BATCH_NUM=1\n",
    "\n",
    "print(\"LLM_WORKER_MAX_TIME =\", os.environ[\"LLM_WORKER_MAX_TIME\"])\n",
    "print(\"MAX_NEW_TOKENS =\", os.environ[\"MAX_NEW_TOKENS\"])\n",
    "print(\"BATCH_NUM =\", os.environ[\"BATCH_NUM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87fe3184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Finished translating dataset\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating:   0%|          | 0/1 [00:00<?, ?it/s]g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\llm_backends.py:73: UserWarning: AutoTokenizer.from_pretrained failed with args {'use_fast': False, 'local_files_only': True}: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "  warnings.warn(f\"AutoTokenizer.from_pretrained failed with args {attempt_kwargs}: {e}\")\n",
      "g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\llm_backends.py:73: UserWarning: AutoTokenizer.from_pretrained failed with args {'use_fast': True, 'local_files_only': True}: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "  warnings.warn(f\"AutoTokenizer.from_pretrained failed with args {attempt_kwargs}: {e}\")\n",
      "g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\llm_backends.py:73: UserWarning: AutoTokenizer.from_pretrained failed with args {'local_files_only': True}: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "  warnings.warn(f\"AutoTokenizer.from_pretrained failed with args {attempt_kwargs}: {e}\")\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "\n",
      "\n",
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[AXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://huggingface.co/aisingapore/Gemma-SEA-LION-v3-9B-IT/resolve/c3e627060b4e450caeedea1e5dab7afba0e01579/model-00002-of-00004.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/aisingapore/Gemma-SEA-LION-v3-9B-IT/resolve/c3e627060b4e450caeedea1e5dab7afba0e01579/model-00003-of-00004.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/aisingapore/Gemma-SEA-LION-v3-9B-IT/resolve/c3e627060b4e450caeedea1e5dab7afba0e01579/model-00004-of-00004.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/aisingapore/Gemma-SEA-LION-v3-9B-IT/resolve/c3e627060b4e450caeedea1e5dab7afba0e01579/model-00001-of-00004.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "\n",
      "\n",
      "Fetching 4 files:  25%|██▌       | 1/4 [27:21<1:22:05, 1641.69s/it]\u001b[A\n",
      "\n",
      "Fetching 4 files:  50%|█████     | 2/4 [27:45<23:00, 690.14s/it]   \u001b[A\n",
      "Fetching 4 files: 100%|██████████| 4/4 [27:45<00:00, 416.44s/it]\n",
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.39s/it]\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 examples from dev split.\n",
      "Processing record ID: ProntoQA_1 with prompt:\n",
      "Translate the following data point from English into Bahasa Indonesia.\n",
      "\n",
      "IMPORTANT INSTRUCTIONS:\n",
      "1) Output plain text only and nothing else.\n",
      "2) Use these keys exactly (do NOT translate the keys): context, question, explanation.\n",
      "3) Translate only the values (text) into Bahasa Indonesia. Do NOT change the keys.\n",
      "4) 'explanation' should be an array/list of short sentences (translated to Bahasa), in the same order as the original explanation.\n",
      "5) Do not add extra fields.\n",
      "\n",
      "INPUT:\n",
      "\n",
      "Context:\n",
      "Jompuses are not shy. Jompuses are yumpuses. Each yumpus is aggressive. Each yumpus is a dumpus. Dumpuses are not wooden. Dumpuses are wumpuses. Wumpuses are red. Every wumpus is an impus. Each impus is opaque. Impuses are tumpuses. Numpuses are sour. Tumpuses are not sour. Tumpuses are vumpuses. Vumpuses are earthy. Every vumpus is a zumpus. Zumpuses are small. Zumpuses are rompuses. Max is a yumpus.\n",
      "\n",
      "Question:\n",
      "Is the following statement true or false? Max is sour.\n",
      "\n",
      "Explanations:\n",
      "Max is a yumpus.\n",
      "Each yumpus is a dumpus.\n",
      "Max is a dumpus.\n",
      "Dumpuses are wumpuses.\n",
      "Max is a wumpus.\n",
      "Every wumpus is an impus.\n",
      "Max is an impus.\n",
      "Impuses are tumpuses.\n",
      "Max is a tumpus.\n",
      "Tumpuses are not sour.\n",
      "Max is not sour.\n",
      "\n",
      "Now translate and output as structured\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python translate_dataset.py --dataset_name ProntoQA --split dev --sample_pct 0 --model_name $LLM_MODEL --max_new_tokens $MAX_NEW_TOKENS --batch_num $BATCH_NUM\n",
    "print(\"\\nFinished translating dataset\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
