# .env.template - copy to .env and edit values

# Backend selection: "ollama", "hf", or "openai"
LLM_BACKEND=hf

# Model id/name to call for the chosen backend
# - Ollama: use the ollama model name you installed (e.g. "sahabatai/your-small")
# - HF: HF model id (e.g. "gpt-oss/some-small-model")
# - OpenAI: OpenAI model id (e.g. "gpt-3.5-turbo")
LLM_MODEL=Sahabat-AI/llama3-8b-cpt-sahabatai-v1-instruct

# Ollama URL if not default
OLLAMA_URL=http://127.0.0.1:11434

# On HF, enable 4-bit quantization (recommended for 8GB GPUs) - set to "1" or "0"
LLM_LOAD_IN_4BIT=1

# Offload folder used by HF loader (optional)
LLM_OFFLOAD_FOLDER=./offload

# OpenAI key if using openai backend
OPENAI_API_KEY=

# Dataset & pipeline options
DATASET_NAME=ProofWriter
SPLIT=dev
DATA_PATH=./data
DATA_SPLIT_PERCENTAGE=10 # Use only N% of the dataset (1-100)
PROMPTS_PATH=./prompts
RESULTS_PATH=./results

# Run options
BATCH_NUM=1
MAX_NEW_TOKENS=512
SEARCH_ROUND=10
USE_MODEL_FOR_NEGATION=false    # "true" or "false" â€” whether negate.py should call the LLM
PILOT_ONLY=true                # If true, run only a single-example pilot (if available)
