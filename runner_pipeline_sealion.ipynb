{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa29a44e",
   "metadata": {},
   "source": [
    "### SEALION-v3-8B-Instruct LLM run on Aristotle pipeline\n",
    "### This is run in Runpod/remote Jupyter environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers safetensors sentencepiece huggingface-hub accelerate bitsandbytes tqdm openai backoff retrying protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/aisingapore/Llama-SEA-LION-v3-8B-IT LLM_MODELS/Llama-SEA-LION-v3-8B-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL_MODEL_PATH = Sahabat-AI/llama3-8b-cpt-sahabatai-v1-instruct\n",
      "LLM_MODEL = Sahabat-AI/llama3-8b-cpt-sahabatai-v1-instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "######### Also useful to reduce thread contention:\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "snapshot_path = \"/workspace/LLM_MODELS/Llama-SEA-LION-v3-8B-IT\" ############## <--- Change this based on platform and models\n",
    "#snapshot_path = \"Sahabat-AI/llama3-8b-cpt-sahabatai-v1-instruct\"\n",
    "\n",
    "os.environ[\"LOCAL_MODEL_PATH\"] = snapshot_path\n",
    "os.environ[\"LLM_MODEL\"] = snapshot_path\n",
    "\n",
    "######## enable 4-bit for quant (and bitsandbytes is set up)\n",
    "os.environ[\"LLM_LOAD_IN_4BIT\"] = \"1\"  # or \"0\" to disable quantization\n",
    "print(\"LOCAL_MODEL_PATH =\", os.environ[\"LOCAL_MODEL_PATH\"])\n",
    "print(\"LLM_MODEL =\", os.environ[\"LLM_MODEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805bf9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\llm_backends.py:52: UserWarning: AutoTokenizer.from_pretrained failed with args {'use_fast': False, 'local_files_only': True}: Loaded object is not a callable tokenizer (type=<class 'bool'>)\n",
      "  warnings.warn(f\"AutoTokenizer.from_pretrained failed with args {attempt_kwargs}: {e}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0b51953acf40d983331c9ecc004085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling short test (3s max_time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result length: 127\n",
      "Write a long list of words and sentences: 1. The sun is shining brightly in the sky. 2. The birds are singing sweet melodies. 3\n",
      "Calling short test (10s max_time)...\n",
      "Result length: 364\n",
      "Write a long list of words and sentences: 1. The sun is shining brightly in the sky. 2. The birds are singing sweet melodies. 3. The breeze is blowing gently through the trees. 4. The flowers are blooming beautifully in the garden. 5. The children are playing happily in the park. 6. The water is flowing smoothly down the stream. 7. The mountains are towering maj\n"
     ]
    }
   ],
   "source": [
    "#Test for Max Time generation stopping criteria\n",
    "from llm_backends import HFBackend\n",
    "hb = HFBackend(local_model_path=snapshot_path, quantize_4bit=True)\n",
    "print(\"Calling short test (3s max_time)...\")\n",
    "res = hb.generate(\"Write a long list of words and sentences: \", max_new_tokens=1024, max_time=3.0)\n",
    "print(\"Result length:\", len(res))\n",
    "print(res[:1000])\n",
    "\n",
    "print(\"Calling short test (10s max_time)...\")\n",
    "res = hb.generate(\"Write a long list of words and sentences: \", max_new_tokens=1024, max_time=10.0)\n",
    "print(\"Result length:\", len(res))\n",
    "print(res[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fe3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python run_pipeline.py --dataset_name ProntoQA --sample_pct 0 --batch_size 1\n",
    "#print(\"\\nFinished translating dataset\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LLM_WORKER_MAX_TIME\"] = \"450\"  # Change this every process (translate, decompose, search_resolve), different time limit is needed in seconds.\n",
    "\n",
    "# With original prompts\n",
    "!python translate_to_fol.py --data_path manual_data_translated --dataset_name ProntoQA --sample_pct 100 --prompts_folder manual_prompts_translated --prompts_file translation --split dev --save_path results_translated/prompts_original --model_name /workspace/LLM_MODELS/Llama-SEA-LION-v3-8B-IT --batch_num 1 --max_new_tokens 6700\n",
    "\n",
    "#!python translate_to_fol.py --data_path manual_data_translated --dataset_name ProntoQA --sample_pct 0 --prompts_folder manual_prompts_translated --split dev --save_path results_translated --model_name Sahabat-AI/llama3-8b-cpt-sahabatai-v1-instruct --batch_num 1 --max_new_tokens 6700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187014c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With modified prompts\n",
    "!python translate_to_fol.py --data_path manual_data_translated --dataset_name ProntoQA --sample_pct 0 --prompts_folder manual_prompts_translated --prompts_file translation_modified --split dev --save_path results_translated/prompts_modified --model_name /workspace/LLM_MODELS/Llama-SEA-LION-v3-8B-IT --batch_num 1 --max_new_tokens 6700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10865d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LLM_WORKER_MAX_TIME\"] = \"900\"\n",
    "\n",
    "#!python decompose_to_cnf.py --data_path manual_data_translated --dataset_name ProntoQA --prompts_folder manual_prompts_translated --split dev --save_path results_translated --model_name /workspace/LLM_MODELS/llama3-8b-cpt-sahabatai-v1-instruct --batch_num 1 --max_new_tokens 6700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d376b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python negate.py --dataset_name ProntoQA --save_path results_translated --model_name /workspace/LLM_MODELS/llama3-8b-cpt-sahabatai-v1-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08f84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python search_resolve.py --data_path manual_data_translate --dataset_name ProntoQA --prompts_folder manual_prompts_translated --split dev --save_path results_translated --model_name /workspace/LLM_MODELS/llama3-8b-cpt-sahabatai-v1-instruct --batch_num 1 --negation False --max_new_tokens 6700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0bc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python search_resolve.py --data_path manual_data_translate --dataset_name ProntoQA --prompts_folder manual_prompts_translated --split dev --save_path results_translated --model_name /workspace/LLM_MODELS/llama3-8b-cpt-sahabatai-v1-instruct --batch_num 1 --negation True --max_new_tokens 6700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725ec0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python evaluate.py --dataset_name ProntoQA --save_path results_translated --model_name /workspace/LLM_MODELS/llama3-8b-cpt-sahabatai-v1-instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
