{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686e984b",
   "metadata": {},
   "source": [
    "## This notebook is used to convert the models to EXL2 format to reduce server rent cost by using ExLlamaV2 inference backend to accelerate token/sec generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92909ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, subprocess, shlex\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8782449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXL2 conversion parameters\n",
    "os.environ[\"LOCAL_PATH\"] = \"G:/LLM_MODELS\"\n",
    "LOCAL_PATH = os.environ.get(\"LOCAL_PATH\")\n",
    "\n",
    "# the folder that contains the model\n",
    "os.environ[\"LLM_MODEL\"] = \"llama3-8b-cpt-sahabatai-v1-instruct\"\n",
    "os.environ[\"LLM_MODEL\"] = \"Llama-SEA-LION-v3-8B-IT\"\n",
    "os.environ[\"LLM_MODEL\"] = \"Qwen2.5-7B-Instruct\"\n",
    "LLM_MODEL = os.environ.get(\"LLM_MODEL\")\n",
    "\n",
    "# e.g. 4.25 to approximate nf4\n",
    "os.environ[\"BITS_TARGET\"] = \"4.65\"\n",
    "BITS_TARGET = os.environ.get(\"BITS_TARGET\")\n",
    "\n",
    "# temp folder for jobs\n",
    "TEMP_PATH = f\"./LLM_MODELS_EXL2/{LLM_MODEL}-EXL2-Indonesia-Focus-temp\"\n",
    "os.environ[\"TEMP_PATH\"] = TEMP_PATH\n",
    "TEMP_PATH = os.environ.get(\"TEMP_PATH\")\n",
    "\n",
    "# base output folder for converted models\n",
    "# constructing the path using an f-string\n",
    "OUT_BASE_PATH = f\"./LLM_MODELS_EXL2/{LLM_MODEL}-EXL2-Indonesia-Focus\"\n",
    "os.environ[\"OUT_BASE\"] = OUT_BASE_PATH\n",
    "OUT_BASE = os.environ.get(\"OUT_BASE\")\n",
    "\n",
    "# optional calibration parquet file (leave blank to skip)\n",
    "os.environ[\"CALIB_PATH\"] = \"./misc/ind_corpus.parquet\"\n",
    "CALIB_PATH = os.environ.get(\"CALIB_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3466da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(LOCAL_PATH) / LLM_MODEL\n",
    "# If cloned with repo name instead of nested path, allow both flavors:\n",
    "if not input_dir.exists() and (Path(LOCAL_PATH) / LLM_MODEL.split('/')[-1]).exists():\n",
    "    input_dir = Path(LOCAL_PATH) / LLM_MODEL.split('/')[-1]\n",
    "\n",
    "if not input_dir.exists():\n",
    "    raise FileNotFoundError(f\"Input model folder not found: {input_dir}\")\n",
    "\n",
    "safe_name = LLM_MODEL.replace(\"/\", \"_\")\n",
    "out_dir = Path(OUT_BASE)\n",
    "temp_dir = Path(TEMP_PATH)\n",
    "calib_file = Path(CALIB_PATH)\n",
    "\n",
    "if out_dir.exists():\n",
    "    print(f\"Removing existing output folder: {out_dir}\")\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c3a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model from G:\\LLM_MODELS\\Qwen2.5-7B-Instruct to LLM_MODELS_EXL2\\Qwen2.5-7B-Instruct-EXL2-Indonesia-Focus with bitwidth 4.65\n",
      "Using calibration file: ./misc/ind_corpus.parquet\n"
     ]
    }
   ],
   "source": [
    "print(f\"Converting model from {input_dir} to {out_dir} with bitwidth {BITS_TARGET}\")\n",
    "print(f\"Using calibration file: {CALIB_PATH if CALIB_PATH else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84391d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find convert.py: prefer local convert.py in cwd, otherwise try exllamav2 package\n",
    "import traceback\n",
    "\n",
    "convert_script = Path(\"..\\exllamav2\\convert.py\")\n",
    "if not convert_script.exists():\n",
    "    try:\n",
    "        import exllamav2\n",
    "        pkg_dir = Path(exllamav2.__file__).resolve().parent\n",
    "        candidates = [\n",
    "            pkg_dir / \"convert.py\",\n",
    "            pkg_dir / \"scripts\" / \"convert.py\",\n",
    "            pkg_dir / \"tools\" / \"convert.py\",\n",
    "        ]\n",
    "        found = None\n",
    "        for c in candidates:\n",
    "            if c.exists():\n",
    "                found = c\n",
    "                break\n",
    "        if not found:\n",
    "            # fallback: any convert*.py in package folder\n",
    "            for c in pkg_dir.iterdir():\n",
    "                if c.is_file() and c.name.lower().startswith(\"convert\") and c.suffix == \".py\":\n",
    "                    found = c\n",
    "                    break\n",
    "        if found is None:\n",
    "            raise FileNotFoundError(f\"convert.py not found in exllamav2 package dir {pkg_dir}\")\n",
    "        convert_script = found\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "# allow user override\n",
    "explicit = os.environ.get(\"EXLLAMA_CONVERT\")\n",
    "if explicit:\n",
    "    convert_script = Path(explicit)\n",
    "    if not convert_script.exists():\n",
    "        raise FileNotFoundError(f\"EXLLAMA_CONVERT set but file not found: {convert_script}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881b167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: 'g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\venv\\Scripts\\python.exe' '..\\exllamav2\\convert.py' -i 'G:\\LLM_MODELS\\Qwen2.5-7B-Instruct' -o 'LLM_MODELS_EXL2\\Qwen2.5-7B-Instruct-EXL2-Indonesia-Focus-temp' -b 4.65 -cf 'LLM_MODELS_EXL2\\Qwen2.5-7B-Instruct-EXL2-Indonesia-Focus' -c 'misc\\ind_corpus.parquet' -nr -hb 8\n",
      "\n",
      "== RETURN CODE: 0\n",
      "\n",
      "== STDOUT (first 2000 chars) ==\n",
      "\n",
      "Loading exllamav2_ext extension (JIT)...\n",
      "\n",
      "Building C++/CUDA extension ------------------------------   0% 0:00:00 -:--:--\n",
      "Building C++/CUDA extension ------------------------------   0% 0:00:00 -:--:--\n",
      " -- Created output directory: LLM_MODELS_EXL2\\Qwen2.5-7B-Instruct-EXL2-Indonesia-Focus-temp\n",
      " -- Beginning new job\n",
      " -- Input: G:\\LLM_MODELS\\Qwen2.5-7B-Instruct\n",
      " -- Output: LLM_MODELS_EXL2\\Qwen2.5-7B-Instruct-EXL2-Indonesia-Focus-temp\n",
      " -- Calibration dataset: misc\\ind_corpus.parquet, 100 / 16 rows, 2048 tokens per sample\n",
      " -- Target bits per weight: 4.65 (decoder), 8 (head)\n",
      " -- Max shard size: 8192 MB\n",
      " -- Full model will be compiled to: LLM_MODELS_EXL2\\Qwen2.5-7B-Instruct-EXL2-Indonesia-Focus\n",
      " -- Tokenizing samples (measurement)...\n",
      " -- First 50 tokens of dataset:\n",
      "    'google/LoraxBench:NLI Ketika sedang menggeluti dunia sinetron, Happy menemukan kecintaan pada sastra, yang kemudian ia tuangkan ke dalam dua buku kumpulan cerpen'\n",
      " -- Last 50 tokens of dataset:\n",
      "    'ak kota Jakarta nang manggambaran baso Jakarta juo mampunyoi sabuah tampek untuak manyambuang anak-anak umua sakolah nang mampunyoi kamampuan di ateh'\n",
      " -- Token embeddings (measurement)...\n",
      " -- Measuring quantization impact...\n",
      " -- Layer: model.layers.0 (Attention)\n",
      " -- model.layers.0.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
      " -- model.layers.0.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
      " -- model.layers.0.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.15 bpw\n",
      " -- model.layers.0.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
      " -- model.layers.0.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
      " -- model.layers.0.self_attn.q_proj                    1:4b_32g s4                                        4.14 bpw\n",
      " -- model.layers.0.self_attn.q_proj                    0.1:5b_128g/0.9:4b\n",
      "\n",
      "== STDERR (first 8000 chars) ==\n",
      "\n",
      "\n",
      "\n",
      "Logs written to LLM_MODELS_EXL2\\Qwen2.5-7B-Instruct-EXL2-Indonesia-Focus/convert_*.log\n"
     ]
    }
   ],
   "source": [
    "# build command\n",
    "cmd = [\n",
    "    sys.executable, str(convert_script),\n",
    "    \"-i\", str(input_dir),\n",
    "    \"-o\", str(temp_dir),\n",
    "    \"-b\", str(BITS_TARGET),\n",
    "    \"-cf\", str(out_dir),\n",
    "    \"-c\", str(calib_file),\n",
    "    \"-nr\",\n",
    "    \"-hb\", \"8\"\n",
    "]\n",
    "\n",
    "print(\"Running:\", \" \".join(shlex.quote(x) for x in cmd))\n",
    "proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "print(\"\\n== RETURN CODE:\", proc.returncode)\n",
    "print(\"\\n== STDOUT (first 2000 chars) ==\\n\")\n",
    "print(proc.stdout[:2000])\n",
    "print(\"\\n== STDERR (first 8000 chars) ==\\n\")\n",
    "print(proc.stderr[:8000])\n",
    "\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(out_dir, \"convert_stdout.log\").write_text(proc.stdout, encoding=\"utf-8\")\n",
    "Path(out_dir, \"convert_stderr.log\").write_text(proc.stderr, encoding=\"utf-8\")\n",
    "print(f\"\\nLogs written to {out_dir}/convert_*.log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
