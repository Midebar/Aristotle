{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b0ea1f",
   "metadata": {},
   "source": [
    "# Aggregate results from all LLM used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5520c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8cba2",
   "metadata": {},
   "source": [
    "## SahabatAIv1-Llama-8B-IT-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f52804",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LLM_MODEL\"] = \"/workspace/LLM_MODELS_GGUF/SahabatAI-v1-Llama3-8B-IT-GGUF/llama3-8b-cpt-sahabatai-v1-instruct-q4_k_m.gguf\"\n",
    "LLM_MODEL = os.environ.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52540b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_naive_prompting/prompt_explanations_after_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_naive_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_search_resolve/v3/prompts_refine --model_name $LLM_MODEL --output_path results_translated_evaluation_aristotle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_cnf_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad97bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_fol_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_fol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf8901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\evaluate.py\", line 254, in <module>\n",
      "    evaluate_files(args.dataset_name, args.model_name)\n",
      "  File \"g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\evaluate.py\", line 148, in evaluate_files\n",
      "    file1 = load_json_file(file1_path)\n",
      "  File \"g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\evaluate.py\", line 11, in load_json_file\n",
      "    return json.load(file)\n",
      "  File \"C:\\Users\\mideb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py\", line 293, in load\n",
      "    return loads(fp.read(),\n",
      "  File \"C:\\Users\\mideb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\mideb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 2813)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using the naive prompting method.\n",
      "Evaluating using the naive prompting method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\evaluate.py\", line 254, in <module>\n",
      "    evaluate_files(args.dataset_name, args.model_name)\n",
      "  File \"g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\evaluate.py\", line 148, in evaluate_files\n",
      "    file1 = load_json_file(file1_path)\n",
      "  File \"g:\\Documents\\Schools\\University\\UI\\Sem_12\\TA\\Aristotle\\evaluate.py\", line 11, in load_json_file\n",
      "    return json.load(file)\n",
      "  File \"C:\\Users\\mideb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py\", line 293, in load\n",
      "    return loads(fp.read(),\n",
      "  File \"C:\\Users\\mideb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\mideb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 2857)\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_fol_prolog --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_prolog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dc753",
   "metadata": {},
   "source": [
    "## Qwen2.5-7B-IT-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6fa9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LLM_MODEL\"] = \"/workspace/LLM_MODELS_GGUF/Qwen2.5-7B-Instruct-GGUF/qwen2.5-7b-instruct-q4_k_m.gguf\"\n",
    "LLM_MODEL = os.environ.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0dd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_naive_prompting/prompt_explanations_after_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_tranresults_translated_evaluation_naiveslated_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_naive_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10125af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_search_resolve/v3/prompts_refine --model_name $LLM_MODEL --output_path results_translated_evaluation_aristotle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26899fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_cnf_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82851887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_fol_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_fol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_fol_prolog --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_prolog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc0e6c",
   "metadata": {},
   "source": [
    "## SEALIONv3-Llama-8B-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LLM_MODEL\"] = \"/workspace/LLM_MODELS_GGUF/Llama-SEA-LION-v3-8B-IT-GGUF/Llama-SEA-LION-v3-8B-IT-Q4_K_M.gguf\"\n",
    "LLM_MODEL = os.environ.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeba5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_naive_prompting/prompt_explanations_after_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c95e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_naive_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_search_resolve/v3/prompts_refine --model_name $LLM_MODEL --output_path results_translated_evaluation_aristotle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_cnf_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09237a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_fol_prompting/prompt_explanations_before_answer --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_fol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --dataset_name ProntoQA --save_path results_translated_fol_prolog --model_name $LLM_MODEL --evaluation_method naive_prompting --output_path results_translated_evaluation_ablation_prolog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
